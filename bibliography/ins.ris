TY  - CONF
T1  - Phishing detection: A recent intelligent machine learning comparison based on models content and features
A1  - Abdelhamid, Neda
A1  - Thabtah, Fadi
A1  - Abdel-Jaber, Hussein
Y1  - 2017/08//
KW  - Computer Security
KW  - Machine Learning
KW  - Phishing Detection
KW  - Web Threat
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - 2017 IEEE International Conference on Intelligence and Security Informatics: Security and Big Data, ISI 2017
SP  - 72
EP  - 77
SN  - 9781509067275
DO  - 10.1109/ISI.2017.8004877
N2  - In the last decade, numerous fake websites have been developed on the World Wide Web to mimic trusted websites, with the aim of stealing financial assets from users and organizations. This form of online attack is called phishing, and it has cost the online community and the various stakeholders hundreds of million Dollars. Therefore, effective counter measures that can accurately detect phishing are needed. Machine learning (ML) is a popular tool for data analysis and recently has shown promising results in combating phishing when contrasted with classic anti-phishing approaches, including awareness workshops, visualization and legal solutions. This article investigates ML techniques applicability to detect phishing attacks and describes their pros and cons. In particular, different types of ML techniques have been investigated to reveal the suitable options that can serve as anti-phishing tools. More importantly, we experimentally compare large numbers of ML techniques on real phishing datasets and with respect to different metrics. The purpose of the comparison is to reveal the advantages and disadvantages of ML predictive models and to show their actual performance when it comes to phishing attacks. The experimental results show that Covering approach models are more appropriate as anti-phishing solutions, especially for novice users, because of their simple yet effective knowledge bases in addition to their good phishing detection rate.
ER  - 
TY  - JOUR
T1  - A New Feature Selection Technique for Load and Price Forecast of Electrical Power Systems
A1  - Abedinia, Oveis
A1  - Amjady, Nima
A1  - Zareipour, Hamidreza
Y1  - 2017/01//
JF  - IEEE Transactions on Power Systems
VL  - 32
IS  - 1
SP  - 62
EP  - 74
DO  - 10.1109/TPWRS.2016.2556620
UR  - http://ieeexplore.ieee.org/document/7458187/
ER  - 
TY  - JOUR
T1  - A feature reduced intrusion detection system using ANN classifier
A1  - Akashdeep
A1  - Manzoor, Ishfaq
A1  - Kumar, Neeraj
Y1  - 2017/12//
KW  - ANN
KW  - Feature Ranking
KW  - Feature Reduction
KW  - Intrusion Detection System (IDS)
JF  - Expert Systems with Applications
VL  - 88
SP  - 249
EP  - 257
DO  - 10.1016/j.eswa.2017.07.005
UR  - https://linkinghub.elsevier.com/retrieve/pii/S0957417417304748
N2  - Rapid increase in internet and network technologies has led to considerable increase in number of attacks and intrusions. Detection and prevention of these attacks has become an important part of security. Intrusion detection system is one of the important ways to achieve high security in computer networks and used to thwart different attacks. Intrusion detection systems have curse of dimensionality which tends to increase time complexity and decrease resource utilization. As a result, it is desirable that important features of data must be analyzed by intrusion detection system to reduce dimensionality. This work proposes an intelligent system which first performs feature ranking on the basis of information gain and correlation. Feature reduction is then done by combining ranks obtained from both information gain and correlation using a novel approach to identify useful and useless features. These reduced features are then fed to a feed forward neural network for training and testing on KDD99 dataset. Pre-processing of KDD-99 dataset has been done to normalize number of instances of each class before training. The system then behaves intelligently to classify test data into attack and non-attack classes. The aim of the feature reduced system is to achieve same degree of performance as a normal system. The system is tested on five different test datasets and both individual and average results of all datasets are reported. Comparison of proposed method with and without feature reduction is done in terms of various performance metrics. Comparisons with recent and relevant approaches are also tabled. Results obtained for proposed method are really encouraging.
ER  - 
TY  - CONF
T1  - The effect of combining different feature selection methods on arabic text classification
A1  - Al-Thubaity, Abdulmohsen
A1  - Abanumay, Norah
A1  - Al-Jerayyed, Sara
A1  - Alrukban, Aljoharah
A1  - Mannaa, Zarah
Y1  - 2013///
KW  - Arabic text classification
KW  - classification accuracy
KW  - classification algorithms
KW  - feature representation
KW  - feature selection
JF  - SNPD 2013 - 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing
SP  - 211
EP  - 216
SN  - 9780769550053
DO  - 10.1109/SNPD.2013.89
N2  - Feature selection is one of several factors affecting text classification systems. Feature selection aims to choose a representative subset of all features to reduce the complexity of classification problems. Usually a single method is used for feature selection. For English, several attempts were reported examining the combination of different feature selection methods. To the best of our knowledge no such attempts were reported for Arabic text classification. In this study, we examined the effect of combining five feature selection methods, namely CHI, IG, GSS, NGL and RS, on Arabic text classification accuracy. Two approaches of combination were used, intersection (AND) and union (OR). The NB classification algorithm was used to classify a Saudi Press Agency dataset which comprised 6,300 texts divided evenly into six classes. Three feature representation schemas were used, namely Boolean, TFiDF and LTC. The experiments show slight improvement in classification accuracy for combining two and three feature selection methods. No improvement on classification accuracy was seen when four or all five feature selection methods were combined. Â© 2013 IEEE.
ER  - 
TY  - ICOMM
T1  - UCI Machine Learning Repository: Data Sets
A1  - Asuncion, A
A1  - Newman, D J
Y1  - 2007///
KW  - data sets
KW  - machine learning
KW  - repositories
JF  - University of California Irvine School of Information
VL  - 2008
UR  - https://archive.ics.uci.edu/ml/datasets.php
UR  - http://www.ics.uci.edu/~mlearn/MLRepository.html%5Cnhttp://archive.ics.uci.edu/ml/datasets.html
N2  - The UCI Machine Learning Repository is a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms. The archive was created as an ftp archive in 1987 by David Aha and fellow graduate students at UC Irvine. Since that time, it has been widely used by students, educators, and researchers all over the world as a primary source of machine learning data sets. As an indication of the impact of the archive, it has been cited over 1000 times, making it one of the top 100 most cited "papers" in all of computer science. The current version of the web site was designed in 2007 by Arthur Asuncion and David Newman, and this project is in collaboration with Rexa.info at the University of Massachusetts Amherst. Funding support from the National Science Foundation is gratefully acknowledged.
ER  - 
TY  - JOUR
T1  - Feature Selection based on Information Gain
A1  - Azhagusundari, B.
A1  - Thanamani, Antony Selvadoss
Y1  - 2013///
KW  - Attribute Reduction
KW  - Discernibility matrix
KW  - Information Gain
JF  - International Journal of Innovative Technology and Exploring Engineering (IJITEE)
VL  - 2
IS  - 2
SP  - 18
EP  - 21
DO  - 2278-3075
UR  - http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.674.9620
L1  - file:///home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Azhagusundari, Thanamani_2013_Feature Selection based on Information Gain.pdf
N2  - The attribute reduction is one of the key processes for knowledge acquisition. Some data set is multidimensional and larger in size. If that data set is used for classification it may end with wrong results and it may also occupy more resources especially in terms of time. Most of the features present are redundant and inconsistent and affect the classification. In order to improve the efficiency of classification these redundancy and inconsistency features must be eliminated. This paper discusses an algorithm based on discernibility matrix and Information gain to reduce attributes.
ER  - 
TY  - JOUR
T1  - A machine learning framework for sport result prediction
A1  - Bunker, Rory P.
A1  - Thabtah, Fadi
Y1  - 2019/01//
KW  - Data mining
KW  - Event forecasting
KW  - Machine learning
KW  - Sport result prediction
JF  - Applied Computing and Informatics
VL  - 15
IS  - 1
SP  - 27
EP  - 33
DO  - 10.1016/j.aci.2017.09.005
UR  - https://linkinghub.elsevier.com/retrieve/pii/S2210832717301485
L1  - file:///home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bunker, Thabtah_2019_A machine learning framework for sport result prediction.pdf
N2  - Machine learning (ML) is one of the intelligent methodologies that have shown promising results in the domains of classification and prediction. One of the expanding areas necessitating good predictive accuracy is sport prediction, due to the large monetary amounts involved in betting. In addition, club managers and owners are striving for classification models so that they can understand and formulate strategies needed to win matches. These models are based on numerous factors involved in the games, such as the results of historical matches, player performance indicators, and opposition information. This paper provides a critical analysis of the literature in ML, focusing on the application of Artificial Neural Network (ANN) to sport results prediction. In doing so, we identify the learning methodologies utilised, data sources, appropriate means of model evaluation, and specific challenges of predicting sport results. This then leads us to propose a novel sport prediction framework through which ML can be used as a learning strategy. Our research will hopefully be informative and of use to those performing future research in this application area.
ER  - 
TY  - JOUR
T1  - A survey on feature selection methods
A1  - Chandrashekar, Girish
A1  - Sahin, Ferat
Y1  - 2014/01//
JF  - Computers and Electrical Engineering
VL  - 40
IS  - 1
SP  - 16
EP  - 28
DO  - 10.1016/j.compeleceng.2013.11.024
N2  - Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques. Â© 2013 Elsevier Ltd. All rights reserved.
ER  - 
TY  - JOUR
T1  - Multinomial Goodness-Of-Fit Tests
A1  - Cressie, Noel
A1  - Read, Timothy R.C.
Y1  - 1984/07//
JF  - Journal of the Royal Statistical Society: Series B (Methodological)
VL  - 46
IS  - 3
SP  - 440
EP  - 464
DO  - 10.1111/j.2517-6161.1984.tb01318.x
UR  - http://doi.wiley.com/10.1111/j.2517-6161.1984.tb01318.x
ER  - 
TY  - JOUR
T1  - Robust Feature Selection for Microarray Data Based on Multicriterion Fusion
A1  - Feng Yang
A1  - Mao, K Z
Y1  - 2011/07//
JF  - IEEE/ACM Transactions on Computational Biology and Bioinformatics
VL  - 8
IS  - 4
SP  - 1080
EP  - 1092
DO  - 10.1109/TCBB.2010.103
UR  - http://ieeexplore.ieee.org/document/5611484/
ER  - 
TY  - JOUR
T1  - Correlation-based Feature Selection for Machine Learning
A1  - Hall, Mark A.
Y1  - 1999///
IS  - April
UR  - http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.4643
L1  - file:///home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall_1999_Correlation-based Feature Selection for Machine Learning.pdf
N2  - A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational ...
ER  - 
TY  - JOUR
T1  - The WEKA data mining software
A1  - Hall, Mark
A1  - Frank, Eibe
A1  - Holmes, Geoffrey
A1  - Pfahringer, Bernhard
A1  - Reutemann, Peter
A1  - Witten, Ian H.
Y1  - 2009/11//
PB  - Association for Computing Machinery (ACM)
JF  - ACM SIGKDD Explorations Newsletter
VL  - 11
IS  - 1
SP  - 10
EP  - 10
DO  - 10.1145/1656274.1656278
N2  - Abstract More than twelve years have elapsed since the first public release of WEKA . In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread ... \n
ER  - 
TY  - JOUR
T1  - Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy
A1  - Hanchuan Peng
A1  - Fuhui Long
A1  - Ding, C.
Y1  - 2005/08//
JF  - IEEE Transactions on Pattern Analysis and Machine Intelligence
VL  - 27
IS  - 8
SP  - 1226
EP  - 1238
DO  - 10.1109/TPAMI.2005.159
UR  - http://ieeexplore.ieee.org/document/1453511/
ER  - 
TY  - JOUR
T1  - Idiot's Bayes - Not so stupid after all?
A1  - Hand, David J.
A1  - Yu, Keming
Y1  - 2001/12//
KW  - Diagnosis
KW  - Independence model
KW  - NaÃ¯ve Bayes
KW  - Simple Bayes
KW  - Supervised classification
JF  - International Statistical Review
VL  - 69
IS  - 3
SP  - 385
EP  - 398
DO  - 10.1111/j.1751-5823.2001.tb00465.x
UR  - http://doi.wiley.com/10.1111/j.1751-5823.2001.tb00465.x
N2  - Folklore has it that a very simple supervised classification rule, based on the typically false assumption that the predictor variables are independent, can be highly effective, and often more effective than sophisticated rules. We examine the evidence for this, both empirical, as observed in real data applications, and theoretical, summarising explanations for why this simple rule might be effective.
ER  - 
TY  - JOUR
T1  - Statistical analysis of features and classification of alphasyllabary sounds in Kannada language
A1  - Hegde, Sarika
A1  - Achary, K. K.
A1  - Shetty, Surendra
Y1  - 2014///
KW  - Alphasyllabary
KW  - Hidden Markov model
KW  - Kannada language
KW  - Linear predictive coding (LPC)
KW  - Mel frequency cepstral coefficients (MFCC)
KW  - Speech recognition
KW  - Statistical analysis
KW  - Support vector machine
KW  - Vector quantization (VQ)
PB  - Kluwer Academic Publishers
JF  - International Journal of Speech Technology
VL  - 18
IS  - 1
SP  - 65
EP  - 75
DO  - 10.1007/s10772-014-9250-8
N2  - Automatic speech recognition (ASR) for a given audio file is a challenging task due to the variations in the type of speech input. Variations may be the environment, language spoken, emotions of the speaker, age/gender of speaker etc. The two main steps in ASR are converting the audio file into features and classifying it appropriately. Basic unit of speech sound is phoneme and the list of such phoneme is language dependent. In Indian languages, basic unit of language is known as Akshara i.e the alphabet. It is known to be an alphasyllabary unit. In our work, we have analyzed the behavior of the acoustic features like, Mel frequency cepstral coefficients and linear predictive coding for various aksharas using techniques like, visualization, probability density function (pdf), QâQ plot and F-ratio. The classifiers, support vector machine (SVM) and hidden Markov model (HMM) are used for classifying the recorded audio into corresponding aksharas. We have also compared the classification performance of HMM and SVM.
ER  - 
TY  - JOUR
T1  - MIFS-ND: A mutual information-based feature selection method
A1  - Hoque, N.
A1  - Bhattacharyya, D.K.
A1  - Kalita, J.K.
Y1  - 2014/10//
JF  - Expert Systems with Applications
VL  - 41
IS  - 14
SP  - 6371
EP  - 6385
DO  - 10.1016/j.eswa.2014.04.019
UR  - https://linkinghub.elsevier.com/retrieve/pii/S0957417414002164
ER  - 
TY  - JOUR
T1  - Performance of feature-selection methods in the classification of high-dimension data
A1  - Hua, Jianping
A1  - Tembe, Waibhav D.
A1  - Dougherty, Edward R.
Y1  - 2009/03//
JF  - Pattern Recognition
VL  - 42
IS  - 3
SP  - 409
EP  - 424
DO  - 10.1016/j.patcog.2008.08.001
UR  - https://linkinghub.elsevier.com/retrieve/pii/S0031320308003142
ER  - 
TY  - CONF
T1  - Sensitivity Analysis for Feature Selection
A1  - Kamalov, Firuz
Y1  - 2019/01//
KW  - Big data
KW  - Feature selection
KW  - Sensitivity analysis
KW  - Total sensitivity index
PB  - Institute of Electrical and Electronics Engineers Inc.
JF  - Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018
SP  - 1466
EP  - 1470
SN  - 9781538668047
DO  - 10.1109/ICMLA.2018.00238
N2  - Sensitivity analysis allows us to decompose the variance output into its source components. Total sensitivity index represents the effects of varying a feature on the variance of the target variable. In this paper we use total sensitivity index to evaluate features for the purpose of feature selection. We test our method on various data sets and compare its performance relative to other modern feature selection methods. The proposed method produces very robust results with high computational efficiency.
ER  - 
TY  - JOUR
T1  - A Feature Selection Method Based on Ranked Vector Scores of Features for Classification
A1  - Kamalov, Firuz
A1  - Thabtah, Fadi
Y1  - 2017/12//
PB  - Springer Science and Business Media LLC
JF  - Annals of Data Science
VL  - 4
IS  - 4
SP  - 483
EP  - 502
DO  - 10.1007/s40745-017-0116-1
N2  - One of the major aspects of any classification process is selecting the rel-evant set of features to be used in a classification algorithm. This initial step in data analysis is called the feature selection process. Disposing of the irrelevant features from the dataset will reduce the complexity of the classification task and will increase the robustness of the decision rules when applied on the test set. This paper proposes a new filtering method that combines and normalizes the scores of three major feature selection methods: information gain, chi-squared statistic and inter-correlation. Our method utilizes the strengths of each of the aforementioned methods to maximum advantage while avoiding their drawbacksâespecially the disparity of the results pro-duced by these methods. Our filtering method stabilizes each variable score and gives it the true rank among the input data's available variables. Hence it maximizes the stability in the variables' scores without losing the overall accuracy of the predictive model. A number of experiments on different datasets from various domains have shown that features chosen by the proposed method are highly predictive when com-pared with features selected by other existing filtering methods. The evaluation of the filtering phase was conducted via thorough experimentations using a number of predictive classification algorithms in addition to statistical analysis of the filtering methods' scores.
ER  - 
TY  - JOUR
T1  - Exploratory Analysis of Feature Selection Techniques in Medical Image Processing
A1  - Kunasekaran, Kokula Krishna Hari
A1  - Sugumaran, Rajkumar
Y1  - 2016///
KW  - cbir
KW  - feature selection
KW  - medical image
KW  - scanning
KW  - screening
KW  - selecting
JF  - International Conference on Information Engineering, Management and Security
VL  - 2016
IS  - Iciems
SP  - 33
EP  - 37
SN  - 9788192986647
UR  - https://www.semanticscholar.org/paper/Exploratory-Analysis-of-Feature-Selection-in-Image-Kunasekaran-Sugumaran/ce4c8cf20ca1ba650efecbb396347fd9b814f7f9
L1  - file:///mnt/sdb1/research/ins.papers/ICIEMS2016007.pdf
ER  - 
TY  - JOUR
T1  - Jointly informative feature selection made tractable by Gaussian modeling
A1  - Lefakis, Leonidas
A1  - Fleuret, FranÃ§ois
Y1  - 2016///
KW  - Entropy
KW  - Feature selection
KW  - Mixture of Gaussians
KW  - Mutual information
JF  - Journal of Machine Learning Research
VL  - 17
SP  - 1
EP  - 39
L1  - file:///mnt/sdb1/research/ins.papers/15-026.pdf
N2  - We address the problem of selecting groups of jointly informative, continuous, features in the context of classification and propose several novel criteria for performing this selection. The proposed class of methods is based on combining a Gaussian modeling of the feature responses with derived bounds on and approximations to their mutual information with the class label. Furthermore, specific algorithmic implementations of these criteria are presented which reduce the computational complexity of the proposed feature selection algorithms by up to two-orders of magnitude. Consequently we show that feature selection based on the joint mutual information of features and class label is in fact tractable; this runs contrary to prior works that largely depend on marginal quantities. An empirical evaluation using several types of classifiers on multiple data sets show that this class of methods outperforms state-of-the-art baselines, both in terms of speed and classification accuracy.
ER  - 
TY  - JOUR
T1  - Feature Selection
A1  - Li, Jundong
A1  - Cheng, Kewei
A1  - Wang, Suhang
A1  - Morstatter, Fred
A1  - Trevino, Robert P.
A1  - Tang, Jiliang
A1  - Liu, Huan
Y1  - 2017/12//
JF  - ACM Computing Surveys
VL  - 50
IS  - 6
SP  - 1
EP  - 45
DO  - 10.1145/3136625
UR  - http://dl.acm.org/citation.cfm?doid=3161158.3136625
ER  - 
TY  - JOUR
T1  - Improving feature selection performance using pairwise pre-evaluation
A1  - Li, Songlu
A1  - Oh, Sejong
Y1  - 2016/12//
JF  - BMC Bioinformatics
VL  - 17
IS  - 1
SP  - 312
EP  - 312
DO  - 10.1186/s12859-016-1178-3
UR  - http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1178-3
ER  - 
TY  - CONF
T1  - Chi2: feature selection and discretization of numeric attributes
A1  - Liu, Huan
A1  - Setiono, Rudy
Y1  - 1995///
PB  - IEEE
JF  - Proceedings of the International Conference on Tools with Artificial Intelligence
SP  - 388
EP  - 391
DO  - 10.1109/tai.1995.479783
N2  - Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant attributes. This paper describes Chi2, a simple and general algorithm that uses the X2 statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data, and achieves feature selection via discretization. The empirical results demonstrate that Chi2 is effective in feature selection and discretization of numeric and ordinal attributes.
ER  - 
TY  - JOUR
T1  - Semi-greedy heuristics for feature selection with test cost constraints
A1  - Min, Fan
A1  - Xu, Juan
Y1  - 2016/09//
PB  - Springer Science and Business Media LLC
JF  - Granular Computing
VL  - 1
IS  - 3
SP  - 199
EP  - 211
DO  - 10.1007/s41066-016-0017-2
L1  - file:///home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Min, Xu_2016_Semi-greedy heuristics for feature selection with test cost constraints.pdf
ER  - 
TY  - JOUR
T1  - Intelligent rule-based phishing websites classification
A1  - Mohammad, Rami M.
A1  - Thabtah, Fadi
A1  - McCluskey, Lee
Y1  - 2014/05//
JF  - IET Information Security
VL  - 8
IS  - 3
SP  - 153
EP  - 160
DO  - 10.1049/iet-ifs.2013.0202
UR  - https://digital-library.theiet.org/content/journals/10.1049/iet-ifs.2013.0202
N2  - Phishing is described as the art of echoing a website of a creditable firm intending to grab user's private information such as usernames, passwords and social security number. Phishing websites comprise a variety of cues within its content-parts as well as the browser-based security indicators provided along with the website. Several solutions have been proposed to tackle phishing. Nevertheless, there is no single magic bullet that can solve this threat radically. One of the promising techniques that can be employed in predicting phishing attacks is based on data mining, particularly the 'induction of classification rules' since anti-phishing solutions aim to predict the website class accurately and that exactly matches the data mining classification technique goals. In this study, the authors shed light on the important features that distinguish phishing websites from legitimate ones and assess how good rule-based data mining classification techniques are in predicting phishing websites and which classification technique is proven to be more reliable. Â© The Institution of Engineering and Technology 2014.
ER  - 
TY  - JOUR
T1  - Induction of decision trees
A1  - Quinlan, J. R.
Y1  - 1986/03//
PB  - Springer Nature
JF  - Machine Learning
VL  - 1
IS  - 1
SP  - 81
EP  - 106
DO  - 10.1007/bf00116251
L1  - file:///home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan_1986_Induction of decision trees.pdf
N2  - Recent literature has demonstrated the applicability of genetic programming to induction of decision trees for modelling toxicity endpoints. Compared with other decision tree induction techniques that are based upon recursive partitioning employing greedy searches to choose the best splitting attribute and value at each node that will necessarily miss regions of the search space, the genetic programming based approach can overcome the problem. However, the method still requires the discretization of the often continuous-valued toxicity endpoints prior to the tree induction. A novel extension of this method, YAdapt, is introduced in this work which models the original continuous endpoint by adaptively finding suitable ranges to describe the endpoints during the tree induction process, removing the need for discretization prior to tree induction and allowing the ordinal nature of the endpoint to be taken into account in the models built.
ER  - 
TY  - JOUR
T1  - New Hybrid Features Selection Method: A Case Study on Websites Phishing
A1  - Rajab, Khairan D.
Y1  - 2017///
JF  - Security and Communication Networks
VL  - 2017
SP  - 1
EP  - 10
DO  - 10.1155/2017/9838169
UR  - https://www.hindawi.com/journals/scn/2017/9838169/
N2  - Phishing is one of the serious web threats that involves mimicking authenticated websites to deceive users in order to obtain their financial information. Phishing has caused financial damage to the different online stakeholders. It is massive in the magnitude of hundreds of millions; hence it is essential to minimize this risk. Classifying websites into âphishyâ and legitimate types is a primary task in data mining that security experts and decision makers are hoping to improve particularly with respect to the detection rate and reliability of the results. One way to ensure the reliability of the results and to enhance performance is to identify a set of related features early on so the data dimensionality reduces and irrelevant features are discarded. To increase reliability of preprocessing, this article proposes a new feature selection method that combines the scores of multiple known methods to minimize discrepancies in feature selection results. The proposed method has been applied to the problem of website phishing classification to show its pros and cons in identifying relevant features. Results against a security dataset reveal that the proposed preprocessing method was able to derive new features datasets which when mined generate high competitive classifiers with reference to detection rate when compared to results obtained from other features selection methods.
ER  - 
TY  - JOUR
T1  - C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993
A1  - Salzberg, Steven L.
Y1  - 1994/09//
PB  - Springer Science and Business Media LLC
JF  - Machine Learning
VL  - 16
IS  - 3
SP  - 235
EP  - 240
DO  - 10.1007/bf00993309
L1  - file:///home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salzberg_1994_C4.5 Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993.pdf
N2  - Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available for download (see below).C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies.This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.
ER  - 
TY  - JOUR
T1  - Artificial neural networks as speech recognisers for dysarthric speech: Identifying the best-performing set of MFCC parameters and studying a speaker-independent approach
A1  - Shahamiri, Seyed Reza
A1  - Binti Salim, Siti Salwah
Y1  - 2014/01//
KW  - Artificial neural network
KW  - Automatic speech recognition
KW  - Dysarthria
KW  - Mel-frequency cepstral coefficients
JF  - Advanced Engineering Informatics
VL  - 28
IS  - 1
SP  - 102
EP  - 110
DO  - 10.1016/j.aei.2014.01.001
N2  - Dysarthria is a neurological impairment of controlling the motor speech articulators that compromises the speech signal. Automatic Speech Recognition (ASR) can be very helpful for speakers with dysarthria because the disabled persons are often physically incapacitated. Mel-Frequency Cepstral Coefficients (MFCCs) have been proven to be an appropriate representation of dysarthric speech, but the question of which MFCC-based feature set represents dysarthric acoustic features most effectively has not been answered. Moreover, most of the current dysarthric speech recognisers are either speaker-dependent (SD) or speaker-adaptive (SA), and they perform poorly in terms of generalisability as a speaker-independent (SI) model. First, by comparing the results of 28 dysarthric SD speech recognisers, this study identifies the best-performing set of MFCC parameters, which can represent dysarthric acoustic features to be used in Artificial Neural Network (ANN)-based ASR. Next, this paper studies the application of ANNs as a fixed-length isolated-word SI ASR for individuals who suffer from dysarthria. The results show that the speech recognisers trained by the conventional 12 coefficients MFCC features without the use of delta and acceleration features provided the best accuracy, and the proposed SI ASR recognised the speech of the unforeseen dysarthric evaluation subjects with word recognition rate of 68.38%. Â© 2013 Elsevier Ltd. All rights reserved.
ER  - 
TY  - JOUR
T1  - A Multi-Views Multi-Learners Approach Towards Dysarthric Speech Recognition Using Multi-Nets Artificial Neural Networks
A1  - Shahamiri, Seyed Reza
A1  - Salim, Siti Salwah Binti
Y1  - 2014/09//
JF  - IEEE Transactions on Neural Systems and Rehabilitation Engineering
VL  - 22
IS  - 5
SP  - 1053
EP  - 1063
DO  - 10.1109/TNSRE.2014.2309336
UR  - http://ieeexplore.ieee.org/document/6762967/
ER  - 
TY  - JOUR
T1  - Artificial Neural Networks as multi-networks automated test oracle
A1  - Shahamiri, Seyed Reza
A1  - Wan-Kadir, Wan M.N.
A1  - Ibrahim, Suhaimi
A1  - Hashim, Siti Zaiton Mohd
Y1  - 2012/09//
KW  - Artificial Neural Networks
KW  - Automated software testing
KW  - Mutation testing
KW  - Software test oracle
JF  - Automated Software Engineering
VL  - 19
IS  - 3
SP  - 303
EP  - 334
DO  - 10.1007/s10515-011-0094-z
N2  - One of the important issues in software testing is to provide an automated test oracle. Test oracles are reliable sources of how the software under test must operate. In particular, they are used to evaluate the actual results produced by the software. However, in order to generate an automated test oracle, it is necessary to map the input domain to the output domain automatically. In this paper, Multi-Networks Oracles based on Artificial Neural Networks are introduced to handle the mapping automatically. They are an enhanced version of previous ANN-Based Oracles. The proposed model was evaluated by a framework provided by mutation testing and applied to test two industry-sized case studies. In particular, a mutated version of each case study was provided and injected with some faults. Then, a fault-free version of it was developed as a Golden Version to evaluate the capability of the proposed oracle finding the injected faults. Meanwhile, the quality of the proposed oracle is measured by assessing its accuracy, precision, misclassification error and recall. Furthermore, the results of the proposed oracle are compared with former ANN-based Oracles. Accuracy of the proposed oracle was up to 98.93%, and the oracle detected up to 98% of the injected faults. The results of the study show the proposed oracle has better quality and applicability than the previous model. Â© Springer Science+Business Media, LLC 2011.
ER  - 
TY  - JOUR
T1  - Deriving Correlated Sets of Website Features for Phishing Detection: A Computational Intelligence Approach
A1  - Thabtah, Fadi
A1  - Abdelhamid, Neda
Y1  - 2016/12//
JF  - Journal of Information & Knowledge Management
VL  - 15
IS  - 04
SP  - 1650042
EP  - 1650042
DO  - 10.1142/S0219649216500428
UR  - https://www.worldscientific.com/doi/abs/10.1142/S0219649216500428
N2  - Classification is one of the major tasks in data mining which aims to build classifiers for decision making. One of the most recent online threats is phishing, which has caused significant losses to online shoppers, electronic businesses and financial institutions. A common way of phishing is impersonating online websites to deceive online users and steal their financial information. One way to guide the anti-phishing classification method is to preliminarily identify a minimal set of related features so the search space can be reduced. The aim of this paper is to compare different features assessment techniques in the website phishing context in order to determine the minimal set of features for detecting phishing activities. Experimental results on real phishing datasets consisting of 30 features has been conducted using three known features selection methods. New features cutoffs have been identified after statistical analysis utilising three data mining classification methods. We have been able to identify new clusters of features that when used together are able to detect phishing activities. Further, important correlations among common features have been derived.
ER  - 
TY  - JOUR
T1  - A new computational intelligence approach to detect autistic features for autism screening
A1  - Thabtah, Fadi
A1  - Kamalov, Firuz
A1  - Rajab, Khairan
Y1  - 2018/09//
JF  - International Journal of Medical Informatics
VL  - 117
SP  - 112
EP  - 124
DO  - 10.1016/j.ijmedinf.2018.06.009
UR  - https://linkinghub.elsevier.com/retrieve/pii/S1386505618300546
ER  - 
TY  - JOUR
T1  - A new machine learning model based on induction of rules for autism detection
A1  - Thabtah, Fadi
A1  - Peebles, David
Y1  - 2019/01//
JF  - Health Informatics Journal
SP  - 146045821882471
EP  - 146045821882471
DO  - 10.1177/1460458218824711
UR  - http://journals.sagepub.com/doi/10.1177/1460458218824711
ER  - 
TY  - JOUR
T1  - A two-stage feature selection method for text categorization by using information gain, principal component analysis and genetic algorithm
A1  - UÄuz, Harun
Y1  - 2011/10//
JF  - Knowledge-Based Systems
VL  - 24
IS  - 7
SP  - 1024
EP  - 1032
DO  - 10.1016/j.knosys.2011.04.014
UR  - https://linkinghub.elsevier.com/retrieve/pii/S0950705111000803
ER  - 
TY  - JOUR
T1  - Maximum weight and minimum redundancy: A novel framework for feature subset selection
A1  - Wang, Jianzhong
A1  - Wu, Lishan
A1  - Kong, Jun
A1  - Li, Yuxin
A1  - Zhang, Baoxue
Y1  - 2013/06//
KW  - Face recognition
KW  - Feature selection
KW  - Maximum weight and minimum redundancy
KW  - Microarray classification
KW  - Text categorization
JF  - Pattern Recognition
VL  - 46
IS  - 6
SP  - 1616
EP  - 1627
DO  - 10.1016/j.patcog.2012.11.025
N2  - Feature subset selection is often required as a preliminary work for many pattern recognition problems. In this paper, a novel filter framework is presented to select optimal feature subset based on a maximum weight and minimum redundancy (MWMR) criterion. Since the weight of each feature indicates its importance for some ad hoc tasks (such as clustering and classification) and the redundancy represents the correlations among features. Through the proposed MWMR, we can select the feature subset in which the features are most beneficial to the subsequent tasks while the redundancy among them is minimal. Moreover, a pair-wise updating based iterative algorithm is introduced to solve our framework effectively. In the experiments, three feature weighting algorithms (Laplacian score, Fisher score and Constraint score) are combined with two redundancy measurement methods (Pearson correlation coefficient and Mutual information) to test the performances of proposed MWMR. The experimental results on five different databases (CMU PIE, Extended YaleB, Colon, DLBCL and PCMAC) demonstrate the advantage and efficiency of our MWMR. Â© 2012 Elsevier Ltd.
ER  - 
TY  - BOOK
T1  - Data Mining: Practical Machine Learning Tools and Techniques, Third Edition (The Morgan Kaufmann Series in Data Management Systems)
A1  - Witten, Ian H
A1  - Frank, Eibe
A1  - Hall, Mark A
Y1  - 2011///
SP  - 664
EP  - 664
SN  - 0123748569
UR  - http://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569%3FSubscriptionId%3D1V7VTJ4HA4MFT9XBJ1R2%26tag%3Dmekentosjcom-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0123748569
N2  - Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization
ER  - 
TY  - JOUR
T1  - Feature Selection Has a Large Impact on One-Class Classification Accuracy for MicroRNAs in Plants
A1  - Yousef, Malik
A1  - SaÃ§ar Demirci, MÃ¼Åerref Duygu
A1  - Khalifa, Waleed
A1  - Allmer, Jens
Y1  - 2016///
JF  - Advances in Bioinformatics
VL  - 2016
SP  - 1
EP  - 6
DO  - 10.1155/2016/5670851
UR  - http://www.hindawi.com/journals/abi/2016/5670851/
N2  - MicroRNAs (miRNAs) are short RNA sequences involved in posttranscriptional gene regulation. Their experimental analysis is complicated and, therefore, needs to be supplemented with computational miRNA detection. Currently computational miRNA detection is mainly performed using machine learning and in particular two-class classification. For machine learning, the miRNAs need to be parametrized and more than 700 features have been described. Positive training examples for machine learning are readily available, but negative data is hard to come by. Therefore, it seems prerogative to use one-class classification instead of two-class classification. Previously, we were able to almost reach two-class classification accuracy using one-class classifiers. In this work, we employ feature selection procedures in conjunction with one-class classification and show that there is up to 36% difference in accuracy among these feature selection methods. The best feature set allowed the training of a one-class classifier which achieved an average accuracy of ~95.6% thereby outperforming previous two-class-based plant miRNA detection approaches by about 0.5%. We believe that this can be improved upon in the future by rigorous filtering of the positive training examples and by improving current feature clustering algorithms to better target pre-miRNA feature selection.
ER  - 
TY  - JOUR
T1  - Feature selection by maximizing correlation information for integrated high-dimensional protein data
A1  - Yuan, Mingshun
A1  - Yang, Zijiang
A1  - Huang, Guangzao
A1  - Ji, Guoli
Y1  - 2017/06//
JF  - Pattern Recognition Letters
VL  - 92
SP  - 17
EP  - 24
DO  - 10.1016/j.patrec.2017.03.011
UR  - https://linkinghub.elsevier.com/retrieve/pii/S0167865517300764
ER  - 
TY  - JOUR
T1  - A Feature Selection Approach Based on Interclass and Intraclass Relative Contributions of Terms
A1  - Zhou, Hongfang
A1  - Guo, Jie
A1  - Wang, Yinghui
A1  - Zhao, Minghua
Y1  - 2016///
JF  - Computational Intelligence and Neuroscience
VL  - 2016
SP  - 1
EP  - 8
DO  - 10.1155/2016/1715780
UR  - http://www.hindawi.com/journals/cin/2016/1715780/
N2  - Feature selection plays a critical role in text categorization. During feature selecting, high-frequency terms and the interclass and intraclass relative contributions of terms all have significant effects on classification results. So we put forward a feature selection approach, IIRCT, based on interclass and intraclass relative contributions of terms in the paper. In our proposed algorithm, three critical factors, which are term frequency and the interclass relative contribution and the intraclass relative contribution of terms, are all considered synthetically. Finally, experiments are made with the help of kNN classifier. And the corresponding results on 20 NewsGroup and SougouCS corpora show that IIRCT algorithm achieves better performance than DF, t -Test, and CMFS algorithms.
ER  - 
