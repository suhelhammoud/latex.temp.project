@article{Akashdeep2017,
abstract = {Rapid increase in internet and network technologies has led to considerable increase in number of attacks and intrusions. Detection and prevention of these attacks has become an important part of security. Intrusion detection system is one of the important ways to achieve high security in computer networks and used to thwart different attacks. Intrusion detection systems have curse of dimensionality which tends to increase time complexity and decrease resource utilization. As a result, it is desirable that important features of data must be analyzed by intrusion detection system to reduce dimensionality. This work proposes an intelligent system which first performs feature ranking on the basis of information gain and correlation. Feature reduction is then done by combining ranks obtained from both information gain and correlation using a novel approach to identify useful and useless features. These reduced features are then fed to a feed forward neural network for training and testing on KDD99 dataset. Pre-processing of KDD-99 dataset has been done to normalize number of instances of each class before training. The system then behaves intelligently to classify test data into attack and non-attack classes. The aim of the feature reduced system is to achieve same degree of performance as a normal system. The system is tested on five different test datasets and both individual and average results of all datasets are reported. Comparison of proposed method with and without feature reduction is done in terms of various performance metrics. Comparisons with recent and relevant approaches are also tabled. Results obtained for proposed method are really encouraging.},
author = {Akashdeep and Manzoor, Ishfaq and Kumar, Neeraj},
doi = {10.1016/j.eswa.2017.07.005},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {ANN,Feature Ranking,Feature Reduction,Intrusion Detection System (IDS)},
month = {dec},
pages = {249--257},
title = {{A feature reduced intrusion detection system using ANN classifier}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417417304748},
volume = {88},
year = {2017}
}
@article{Zhou2016,
abstract = {Feature selection plays a critical role in text categorization. During feature selecting, high-frequency terms and the interclass and intraclass relative contributions of terms all have significant effects on classification results. So we put forward a feature selection approach, IIRCT, based on interclass and intraclass relative contributions of terms in the paper. In our proposed algorithm, three critical factors, which are term frequency and the interclass relative contribution and the intraclass relative contribution of terms, are all considered synthetically. Finally, experiments are made with the help of kNN classifier. And the corresponding results on 20 NewsGroup and SougouCS corpora show that IIRCT algorithm achieves better performance than DF, t -Test, and CMFS algorithms.},
author = {Zhou, Hongfang and Guo, Jie and Wang, Yinghui and Zhao, Minghua},
doi = {10.1155/2016/1715780},
issn = {1687-5265},
journal = {Computational Intelligence and Neuroscience},
pages = {1--8},
title = {{A Feature Selection Approach Based on Interclass and Intraclass Relative Contributions of Terms}},
url = {http://www.hindawi.com/journals/cin/2016/1715780/},
volume = {2016},
year = {2016}
}
@article{Kamalov2017,
abstract = {One of the major aspects of any classification process is selecting the rel-evant set of features to be used in a classification algorithm. This initial step in data analysis is called the feature selection process. Disposing of the irrelevant features from the dataset will reduce the complexity of the classification task and will increase the robustness of the decision rules when applied on the test set. This paper proposes a new filtering method that combines and normalizes the scores of three major feature selection methods: information gain, chi-squared statistic and inter-correlation. Our method utilizes the strengths of each of the aforementioned methods to maximum advantage while avoiding their drawbacks—especially the disparity of the results pro-duced by these methods. Our filtering method stabilizes each variable score and gives it the true rank among the input data's available variables. Hence it maximizes the stability in the variables' scores without losing the overall accuracy of the predictive model. A number of experiments on different datasets from various domains have shown that features chosen by the proposed method are highly predictive when com-pared with features selected by other existing filtering methods. The evaluation of the filtering phase was conducted via thorough experimentations using a number of predictive classification algorithms in addition to statistical analysis of the filtering methods' scores.},
author = {Kamalov, Firuz and Thabtah, Fadi},
doi = {10.1007/s40745-017-0116-1},
issn = {2198-5804},
journal = {Annals of Data Science},
month = {dec},
number = {4},
pages = {483--502},
publisher = {Springer Science and Business Media LLC},
title = {{A Feature Selection Method Based on Ranked Vector Scores of Features for Classification}},
volume = {4},
year = {2017}
}
@article{Bunker2019,
abstract = {Machine learning (ML) is one of the intelligent methodologies that have shown promising results in the domains of classification and prediction. One of the expanding areas necessitating good predictive accuracy is sport prediction, due to the large monetary amounts involved in betting. In addition, club managers and owners are striving for classification models so that they can understand and formulate strategies needed to win matches. These models are based on numerous factors involved in the games, such as the results of historical matches, player performance indicators, and opposition information. This paper provides a critical analysis of the literature in ML, focusing on the application of Artificial Neural Network (ANN) to sport results prediction. In doing so, we identify the learning methodologies utilised, data sources, appropriate means of model evaluation, and specific challenges of predicting sport results. This then leads us to propose a novel sport prediction framework through which ML can be used as a learning strategy. Our research will hopefully be informative and of use to those performing future research in this application area.},
author = {Bunker, Rory P. and Thabtah, Fadi},
doi = {10.1016/j.aci.2017.09.005},
file = {:home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bunker, Thabtah{\_}2019{\_}A machine learning framework for sport result prediction.pdf:pdf},
issn = {22108327},
journal = {Applied Computing and Informatics},
keywords = {Data mining,Event forecasting,Machine learning,Sport result prediction},
month = {jan},
number = {1},
pages = {27--33},
title = {{A machine learning framework for sport result prediction}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2210832717301485},
volume = {15},
year = {2019}
}
@article{Shahamiri2014a,
author = {Shahamiri, Seyed Reza and Salim, Siti Salwah Binti},
doi = {10.1109/TNSRE.2014.2309336},
issn = {1534-4320},
journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
month = {sep},
number = {5},
pages = {1053--1063},
title = {{A Multi-Views Multi-Learners Approach Towards Dysarthric Speech Recognition Using Multi-Nets Artificial Neural Networks}},
url = {http://ieeexplore.ieee.org/document/6762967/},
volume = {22},
year = {2014}
}
@article{Thabtah2018,
author = {Thabtah, Fadi and Kamalov, Firuz and Rajab, Khairan},
doi = {10.1016/j.ijmedinf.2018.06.009},
issn = {13865056},
journal = {International Journal of Medical Informatics},
month = {sep},
pages = {112--124},
title = {{A new computational intelligence approach to detect autistic features for autism screening}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1386505618300546},
volume = {117},
year = {2018}
}
@article{Abedinia2017,
author = {Abedinia, Oveis and Amjady, Nima and Zareipour, Hamidreza},
doi = {10.1109/TPWRS.2016.2556620},
issn = {0885-8950},
journal = {IEEE Transactions on Power Systems},
month = {jan},
number = {1},
pages = {62--74},
title = {{A New Feature Selection Technique for Load and Price Forecast of Electrical Power Systems}},
url = {http://ieeexplore.ieee.org/document/7458187/},
volume = {32},
year = {2017}
}
@article{Thabtah2019,
author = {Thabtah, Fadi and Peebles, David},
doi = {10.1177/1460458218824711},
issn = {1460-4582},
journal = {Health Informatics Journal},
month = {jan},
pages = {146045821882471},
title = {{A new machine learning model based on induction of rules for autism detection}},
url = {http://journals.sagepub.com/doi/10.1177/1460458218824711},
year = {2019}
}
@article{Chandrashekar2014,
abstract = {Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Chandrashekar, Girish and Sahin, Ferat},
doi = {10.1016/j.compeleceng.2013.11.024},
issn = {00457906},
journal = {Computers and Electrical Engineering},
month = {jan},
number = {1},
pages = {16--28},
title = {{A survey on feature selection methods}},
volume = {40},
year = {2014}
}
@article{Uguz2011,
author = {Uğuz, Harun},
doi = {10.1016/j.knosys.2011.04.014},
issn = {09507051},
journal = {Knowledge-Based Systems},
month = {oct},
number = {7},
pages = {1024--1032},
title = {{A two-stage feature selection method for text categorization by using information gain, principal component analysis and genetic algorithm}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705111000803},
volume = {24},
year = {2011}
}
@article{Shahamiri2012,
abstract = {One of the important issues in software testing is to provide an automated test oracle. Test oracles are reliable sources of how the software under test must operate. In particular, they are used to evaluate the actual results produced by the software. However, in order to generate an automated test oracle, it is necessary to map the input domain to the output domain automatically. In this paper, Multi-Networks Oracles based on Artificial Neural Networks are introduced to handle the mapping automatically. They are an enhanced version of previous ANN-Based Oracles. The proposed model was evaluated by a framework provided by mutation testing and applied to test two industry-sized case studies. In particular, a mutated version of each case study was provided and injected with some faults. Then, a fault-free version of it was developed as a Golden Version to evaluate the capability of the proposed oracle finding the injected faults. Meanwhile, the quality of the proposed oracle is measured by assessing its accuracy, precision, misclassification error and recall. Furthermore, the results of the proposed oracle are compared with former ANN-based Oracles. Accuracy of the proposed oracle was up to 98.93{\%}, and the oracle detected up to 98{\%} of the injected faults. The results of the study show the proposed oracle has better quality and applicability than the previous model. {\textcopyright} Springer Science+Business Media, LLC 2011.},
author = {Shahamiri, Seyed Reza and Wan-Kadir, Wan M.N. and Ibrahim, Suhaimi and Hashim, Siti Zaiton Mohd},
doi = {10.1007/s10515-011-0094-z},
issn = {09288910},
journal = {Automated Software Engineering},
keywords = {Artificial Neural Networks,Automated software testing,Mutation testing,Software test oracle},
month = {sep},
number = {3},
pages = {303--334},
title = {{Artificial Neural Networks as multi-networks automated test oracle}},
volume = {19},
year = {2012}
}
@article{Shahamiri2014,
abstract = {Dysarthria is a neurological impairment of controlling the motor speech articulators that compromises the speech signal. Automatic Speech Recognition (ASR) can be very helpful for speakers with dysarthria because the disabled persons are often physically incapacitated. Mel-Frequency Cepstral Coefficients (MFCCs) have been proven to be an appropriate representation of dysarthric speech, but the question of which MFCC-based feature set represents dysarthric acoustic features most effectively has not been answered. Moreover, most of the current dysarthric speech recognisers are either speaker-dependent (SD) or speaker-adaptive (SA), and they perform poorly in terms of generalisability as a speaker-independent (SI) model. First, by comparing the results of 28 dysarthric SD speech recognisers, this study identifies the best-performing set of MFCC parameters, which can represent dysarthric acoustic features to be used in Artificial Neural Network (ANN)-based ASR. Next, this paper studies the application of ANNs as a fixed-length isolated-word SI ASR for individuals who suffer from dysarthria. The results show that the speech recognisers trained by the conventional 12 coefficients MFCC features without the use of delta and acceleration features provided the best accuracy, and the proposed SI ASR recognised the speech of the unforeseen dysarthric evaluation subjects with word recognition rate of 68.38{\%}. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Shahamiri, Seyed Reza and {Binti Salim}, Siti Salwah},
doi = {10.1016/j.aei.2014.01.001},
issn = {14740346},
journal = {Advanced Engineering Informatics},
keywords = {Artificial neural network,Automatic speech recognition,Dysarthria,Mel-frequency cepstral coefficients},
month = {jan},
number = {1},
pages = {102--110},
title = {{Artificial neural networks as speech recognisers for dysarthric speech: Identifying the best-performing set of MFCC parameters and studying a speaker-independent approach}},
volume = {28},
year = {2014}
}
@article{Salzberg1994,
abstract = {Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available for download (see below).C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies.This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.},
author = {Salzberg, Steven L.},
doi = {10.1007/bf00993309},
file = {:home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Salzberg{\_}1994{\_}C4.5 Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {sep},
number = {3},
pages = {235--240},
publisher = {Springer Science and Business Media LLC},
title = {{C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993}},
volume = {16},
year = {1994}
}
@inproceedings{Liu1995,
abstract = {Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant attributes. This paper describes Chi2, a simple and general algorithm that uses the X2 statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data, and achieves feature selection via discretization. The empirical results demonstrate that Chi2 is effective in feature selection and discretization of numeric and ordinal attributes.},
author = {Liu, Huan and Setiono, Rudy},
booktitle = {Proceedings of the International Conference on Tools with Artificial Intelligence},
doi = {10.1109/tai.1995.479783},
issn = {10636730},
pages = {388--391},
publisher = {IEEE},
title = {{Chi2: feature selection and discretization of numeric attributes}},
year = {1995}
}
@article{Hall1999,
abstract = {A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational ...},
author = {Hall, Mark A.},
file = {:home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall{\_}1999{\_}Correlation-based Feature Selection for Machine Learning.pdf:pdf},
number = {April},
title = {{Correlation-based Feature Selection for Machine Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.4643},
year = {1999}
}
@book{Witten2011,
abstract = {Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization},
author = {Witten, Ian H and Frank, Eibe and Hall, Mark A},
isbn = {0123748569},
pages = {664},
pmid = {123748569},
title = {{Data Mining: Practical Machine Learning Tools and Techniques, Third Edition (The Morgan Kaufmann Series in Data Management Systems)}},
url = {http://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569{\%}3FSubscriptionId{\%}3D1V7VTJ4HA4MFT9XBJ1R2{\%}26tag{\%}3Dmekentosjcom-20{\%}26linkCode{\%}3Dxm2{\%}26camp{\%}3D2025{\%}26creative{\%}3D165953{\%}26creativeASIN{\%}3D0123748569},
year = {2011}
}
@article{Thabtah2016,
abstract = {Classification is one of the major tasks in data mining which aims to build classifiers for decision making. One of the most recent online threats is phishing, which has caused significant losses to online shoppers, electronic businesses and financial institutions. A common way of phishing is impersonating online websites to deceive online users and steal their financial information. One way to guide the anti-phishing classification method is to preliminarily identify a minimal set of related features so the search space can be reduced. The aim of this paper is to compare different features assessment techniques in the website phishing context in order to determine the minimal set of features for detecting phishing activities. Experimental results on real phishing datasets consisting of 30 features has been conducted using three known features selection methods. New features cutoffs have been identified after statistical analysis utilising three data mining classification methods. We have been able to identify new clusters of features that when used together are able to detect phishing activities. Further, important correlations among common features have been derived.},
author = {Thabtah, Fadi and Abdelhamid, Neda},
doi = {10.1142/S0219649216500428},
issn = {0219-6492},
journal = {Journal of Information {\&} Knowledge Management},
month = {dec},
number = {04},
pages = {1650042},
title = {{Deriving Correlated Sets of Website Features for Phishing Detection: A Computational Intelligence Approach}},
url = {https://www.worldscientific.com/doi/abs/10.1142/S0219649216500428},
volume = {15},
year = {2016}
}
@article{Kunasekaran2016,
author = {Kunasekaran, Kokula Krishna Hari and Sugumaran, Rajkumar},
file = {:mnt/sdb1/research/ins.papers/ICIEMS2016007.pdf:pdf},
isbn = {9788192986647},
journal = {International Conference on Information Engineering, Management and Security},
keywords = {cbir,feature selection,medical image,scanning,screening,selecting},
number = {Iciems},
pages = {33--37},
title = {{Exploratory Analysis of Feature Selection Techniques in Medical Image Processing}},
url = {https://www.semanticscholar.org/paper/Exploratory-Analysis-of-Feature-Selection-in-Image-Kunasekaran-Sugumaran/ce4c8cf20ca1ba650efecbb396347fd9b814f7f9},
volume = {2016},
year = {2016}
}
@article{Li2017,
author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
doi = {10.1145/3136625},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {dec},
number = {6},
pages = {1--45},
title = {{Feature Selection}},
url = {http://dl.acm.org/citation.cfm?doid=3161158.3136625},
volume = {50},
year = {2017}
}
@article{Azhagusundari2013,
abstract = {The attribute reduction is one of the key processes for knowledge acquisition. Some data set is multidimensional and larger in size. If that data set is used for classification it may end with wrong results and it may also occupy more resources especially in terms of time. Most of the features present are redundant and inconsistent and affect the classification. In order to improve the efficiency of classification these redundancy and inconsistency features must be eliminated. This paper discusses an algorithm based on discernibility matrix and Information gain to reduce attributes.},
author = {Azhagusundari, B. and Thanamani, Antony Selvadoss},
doi = {2278-3075},
file = {:home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Azhagusundari, Thanamani{\_}2013{\_}Feature Selection based on Information Gain.pdf:pdf},
journal = {International Journal of Innovative Technology and Exploring Engineering (IJITEE)},
keywords = {Attribute Reduction,Discernibility matrix,Information Gain},
number = {2},
pages = {18--21},
title = {{Feature Selection based on Information Gain}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.674.9620},
volume = {2},
year = {2013}
}
@article{HanchuanPeng2005,
author = {{Hanchuan Peng} and {Fuhui Long} and Ding, C.},
doi = {10.1109/TPAMI.2005.159},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {aug},
number = {8},
pages = {1226--1238},
title = {{Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy}},
url = {http://ieeexplore.ieee.org/document/1453511/},
volume = {27},
year = {2005}
}
@article{Yuan2017,
author = {Yuan, Mingshun and Yang, Zijiang and Huang, Guangzao and Ji, Guoli},
doi = {10.1016/j.patrec.2017.03.011},
issn = {01678655},
journal = {Pattern Recognition Letters},
month = {jun},
pages = {17--24},
title = {{Feature selection by maximizing correlation information for integrated high-dimensional protein data}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865517300764},
volume = {92},
year = {2017}
}
@article{Yousef2016,
abstract = {MicroRNAs (miRNAs) are short RNA sequences involved in posttranscriptional gene regulation. Their experimental analysis is complicated and, therefore, needs to be supplemented with computational miRNA detection. Currently computational miRNA detection is mainly performed using machine learning and in particular two-class classification. For machine learning, the miRNAs need to be parametrized and more than 700 features have been described. Positive training examples for machine learning are readily available, but negative data is hard to come by. Therefore, it seems prerogative to use one-class classification instead of two-class classification. Previously, we were able to almost reach two-class classification accuracy using one-class classifiers. In this work, we employ feature selection procedures in conjunction with one-class classification and show that there is up to 36{\%} difference in accuracy among these feature selection methods. The best feature set allowed the training of a one-class classifier which achieved an average accuracy of {\~{}}95.6{\%} thereby outperforming previous two-class-based plant miRNA detection approaches by about 0.5{\%}. We believe that this can be improved upon in the future by rigorous filtering of the positive training examples and by improving current feature clustering algorithms to better target pre-miRNA feature selection.},
author = {Yousef, Malik and {Sa{\c{c}}ar Demirci}, M{\"{u}}şerref Duygu and Khalifa, Waleed and Allmer, Jens},
doi = {10.1155/2016/5670851},
issn = {1687-8027},
journal = {Advances in Bioinformatics},
pages = {1--6},
title = {{Feature Selection Has a Large Impact on One-Class Classification Accuracy for MicroRNAs in Plants}},
url = {http://www.hindawi.com/journals/abi/2016/5670851/},
volume = {2016},
year = {2016}
}
@article{Hand2001,
abstract = {Folklore has it that a very simple supervised classification rule, based on the typically false assumption that the predictor variables are independent, can be highly effective, and often more effective than sophisticated rules. We examine the evidence for this, both empirical, as observed in real data applications, and theoretical, summarising explanations for why this simple rule might be effective.},
author = {Hand, David J. and Yu, Keming},
doi = {10.1111/j.1751-5823.2001.tb00465.x},
issn = {03067734},
journal = {International Statistical Review},
keywords = {Diagnosis,Independence model,Na{\"{i}}ve Bayes,Simple Bayes,Supervised classification},
month = {dec},
number = {3},
pages = {385--398},
title = {{Idiot's Bayes - Not so stupid after all?}},
url = {http://doi.wiley.com/10.1111/j.1751-5823.2001.tb00465.x},
volume = {69},
year = {2001}
}
@article{Li2016,
author = {Li, Songlu and Oh, Sejong},
doi = {10.1186/s12859-016-1178-3},
issn = {1471-2105},
journal = {BMC Bioinformatics},
month = {dec},
number = {1},
pages = {312},
title = {{Improving feature selection performance using pairwise pre-evaluation}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1178-3},
volume = {17},
year = {2016}
}
@article{Quinlan1986,
abstract = {Recent literature has demonstrated the applicability of genetic programming to induction of decision trees for modelling toxicity endpoints. Compared with other decision tree induction techniques that are based upon recursive partitioning employing greedy searches to choose the best splitting attribute and value at each node that will necessarily miss regions of the search space, the genetic programming based approach can overcome the problem. However, the method still requires the discretization of the often continuous-valued toxicity endpoints prior to the tree induction. A novel extension of this method, YAdapt, is introduced in this work which models the original continuous endpoint by adaptively finding suitable ranges to describe the endpoints during the tree induction process, removing the need for discretization prior to tree induction and allowing the ordinal nature of the endpoint to be taken into account in the models built.},
author = {Quinlan, J. R.},
doi = {10.1007/bf00116251},
file = {:home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quinlan{\_}1986{\_}Induction of decision trees.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {mar},
number = {1},
pages = {81--106},
publisher = {Springer Nature},
title = {{Induction of decision trees}},
volume = {1},
year = {1986}
}
@article{McCluskey2014,
abstract = {Phishing is described as the art of echoing a website of a creditable firm intending to grab user's private information such as usernames, passwords and social security number. Phishing websites comprise a variety of cues within its content-parts as well as the browser-based security indicators provided along with the website. Several solutions have been proposed to tackle phishing. Nevertheless, there is no single magic bullet that can solve this threat radically. One of the promising techniques that can be employed in predicting phishing attacks is based on data mining, particularly the 'induction of classification rules' since anti-phishing solutions aim to predict the website class accurately and that exactly matches the data mining classification technique goals. In this study, the authors shed light on the important features that distinguish phishing websites from legitimate ones and assess how good rule-based data mining classification techniques are in predicting phishing websites and which classification technique is proven to be more reliable. {\textcopyright} The Institution of Engineering and Technology 2014.},
author = {Mohammad, Rami M. and Thabtah, Fadi and McCluskey, Lee},
doi = {10.1049/iet-ifs.2013.0202},
issn = {17518717},
journal = {IET Information Security},
month = {may},
number = {3},
pages = {153--160},
title = {{Intelligent rule-based phishing websites classification}},
url = {https://digital-library.theiet.org/content/journals/10.1049/iet-ifs.2013.0202},
volume = {8},
year = {2014}
}
@article{Lefakis2016,
abstract = {We address the problem of selecting groups of jointly informative, continuous, features in the context of classification and propose several novel criteria for performing this selection. The proposed class of methods is based on combining a Gaussian modeling of the feature responses with derived bounds on and approximations to their mutual information with the class label. Furthermore, specific algorithmic implementations of these criteria are presented which reduce the computational complexity of the proposed feature selection algorithms by up to two-orders of magnitude. Consequently we show that feature selection based on the joint mutual information of features and class label is in fact tractable; this runs contrary to prior works that largely depend on marginal quantities. An empirical evaluation using several types of classifiers on multiple data sets show that this class of methods outperforms state-of-the-art baselines, both in terms of speed and classification accuracy.},
author = {Lefakis, Leonidas and Fleuret, Fran{\c{c}}ois},
file = {:mnt/sdb1/research/ins.papers/15-026.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Entropy,Feature selection,Mixture of Gaussians,Mutual information},
pages = {1--39},
title = {{Jointly informative feature selection made tractable by Gaussian modeling}},
volume = {17},
year = {2016}
}
@article{Wang2013,
abstract = {Feature subset selection is often required as a preliminary work for many pattern recognition problems. In this paper, a novel filter framework is presented to select optimal feature subset based on a maximum weight and minimum redundancy (MWMR) criterion. Since the weight of each feature indicates its importance for some ad hoc tasks (such as clustering and classification) and the redundancy represents the correlations among features. Through the proposed MWMR, we can select the feature subset in which the features are most beneficial to the subsequent tasks while the redundancy among them is minimal. Moreover, a pair-wise updating based iterative algorithm is introduced to solve our framework effectively. In the experiments, three feature weighting algorithms (Laplacian score, Fisher score and Constraint score) are combined with two redundancy measurement methods (Pearson correlation coefficient and Mutual information) to test the performances of proposed MWMR. The experimental results on five different databases (CMU PIE, Extended YaleB, Colon, DLBCL and PCMAC) demonstrate the advantage and efficiency of our MWMR. {\textcopyright} 2012 Elsevier Ltd.},
author = {Wang, Jianzhong and Wu, Lishan and Kong, Jun and Li, Yuxin and Zhang, Baoxue},
doi = {10.1016/j.patcog.2012.11.025},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Face recognition,Feature selection,Maximum weight and minimum redundancy,Microarray classification,Text categorization},
month = {jun},
number = {6},
pages = {1616--1627},
title = {{Maximum weight and minimum redundancy: A novel framework for feature subset selection}},
volume = {46},
year = {2013}
}
@article{Hoque2014,
author = {Hoque, N. and Bhattacharyya, D.K. and Kalita, J.K.},
doi = {10.1016/j.eswa.2014.04.019},
issn = {09574174},
journal = {Expert Systems with Applications},
month = {oct},
number = {14},
pages = {6371--6385},
title = {{MIFS-ND: A mutual information-based feature selection method}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417414002164},
volume = {41},
year = {2014}
}
@article{Cressie1984,
author = {Cressie, Noel and Read, Timothy R.C.},
doi = {10.1111/j.2517-6161.1984.tb01318.x},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
month = {jul},
number = {3},
pages = {440--464},
title = {{Multinomial Goodness-Of-Fit Tests}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1984.tb01318.x},
volume = {46},
year = {1984}
}
@article{Rajab2017,
abstract = {Phishing is one of the serious web threats that involves mimicking authenticated websites to deceive users in order to obtain their financial information. Phishing has caused financial damage to the different online stakeholders. It is massive in the magnitude of hundreds of millions; hence it is essential to minimize this risk. Classifying websites into “phishy” and legitimate types is a primary task in data mining that security experts and decision makers are hoping to improve particularly with respect to the detection rate and reliability of the results. One way to ensure the reliability of the results and to enhance performance is to identify a set of related features early on so the data dimensionality reduces and irrelevant features are discarded. To increase reliability of preprocessing, this article proposes a new feature selection method that combines the scores of multiple known methods to minimize discrepancies in feature selection results. The proposed method has been applied to the problem of website phishing classification to show its pros and cons in identifying relevant features. Results against a security dataset reveal that the proposed preprocessing method was able to derive new features datasets which when mined generate high competitive classifiers with reference to detection rate when compared to results obtained from other features selection methods.},
author = {Rajab, Khairan D.},
doi = {10.1155/2017/9838169},
issn = {1939-0114},
journal = {Security and Communication Networks},
pages = {1--10},
title = {{New Hybrid Features Selection Method: A Case Study on Websites Phishing}},
url = {https://www.hindawi.com/journals/scn/2017/9838169/},
volume = {2017},
year = {2017}
}
@article{Hua2009,
author = {Hua, Jianping and Tembe, Waibhav D. and Dougherty, Edward R.},
doi = {10.1016/j.patcog.2008.08.001},
issn = {00313203},
journal = {Pattern Recognition},
month = {mar},
number = {3},
pages = {409--424},
title = {{Performance of feature-selection methods in the classification of high-dimension data}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320308003142},
volume = {42},
year = {2009}
}
@inproceedings{Abdelhamid2017,
abstract = {In the last decade, numerous fake websites have been developed on the World Wide Web to mimic trusted websites, with the aim of stealing financial assets from users and organizations. This form of online attack is called phishing, and it has cost the online community and the various stakeholders hundreds of million Dollars. Therefore, effective counter measures that can accurately detect phishing are needed. Machine learning (ML) is a popular tool for data analysis and recently has shown promising results in combating phishing when contrasted with classic anti-phishing approaches, including awareness workshops, visualization and legal solutions. This article investigates ML techniques applicability to detect phishing attacks and describes their pros and cons. In particular, different types of ML techniques have been investigated to reveal the suitable options that can serve as anti-phishing tools. More importantly, we experimentally compare large numbers of ML techniques on real phishing datasets and with respect to different metrics. The purpose of the comparison is to reveal the advantages and disadvantages of ML predictive models and to show their actual performance when it comes to phishing attacks. The experimental results show that Covering approach models are more appropriate as anti-phishing solutions, especially for novice users, because of their simple yet effective knowledge bases in addition to their good phishing detection rate.},
author = {Abdelhamid, Neda and Thabtah, Fadi and Abdel-Jaber, Hussein},
booktitle = {2017 IEEE International Conference on Intelligence and Security Informatics: Security and Big Data, ISI 2017},
doi = {10.1109/ISI.2017.8004877},
isbn = {9781509067275},
keywords = {Computer Security,Machine Learning,Phishing Detection,Web Threat},
month = {aug},
pages = {72--77},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Phishing detection: A recent intelligent machine learning comparison based on models content and features}},
year = {2017}
}
@article{FengYang2011,
author = {{Feng Yang} and Mao, K Z},
doi = {10.1109/TCBB.2010.103},
issn = {1545-5963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
month = {jul},
number = {4},
pages = {1080--1092},
title = {{Robust Feature Selection for Microarray Data Based on Multicriterion Fusion}},
url = {http://ieeexplore.ieee.org/document/5611484/},
volume = {8},
year = {2011}
}
@article{Min2016,
author = {Min, Fan and Xu, Juan},
doi = {10.1007/s41066-016-0017-2},
file = {:home/suhel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Min, Xu{\_}2016{\_}Semi-greedy heuristics for feature selection with test cost constraints.pdf:pdf},
issn = {2364-4966},
journal = {Granular Computing},
month = {sep},
number = {3},
pages = {199--211},
publisher = {Springer Science and Business Media LLC},
title = {{Semi-greedy heuristics for feature selection with test cost constraints}},
volume = {1},
year = {2016}
}
@inproceedings{Kamalov2019,
abstract = {Sensitivity analysis allows us to decompose the variance output into its source components. Total sensitivity index represents the effects of varying a feature on the variance of the target variable. In this paper we use total sensitivity index to evaluate features for the purpose of feature selection. We test our method on various data sets and compare its performance relative to other modern feature selection methods. The proposed method produces very robust results with high computational efficiency.},
author = {Kamalov, Firuz},
booktitle = {Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018},
doi = {10.1109/ICMLA.2018.00238},
isbn = {9781538668047},
keywords = {Big data,Feature selection,Sensitivity analysis,Total sensitivity index},
month = {jan},
pages = {1466--1470},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Sensitivity Analysis for Feature Selection}},
year = {2019}
}
@article{Hegde2014,
abstract = {Automatic speech recognition (ASR) for a given audio file is a challenging task due to the variations in the type of speech input. Variations may be the environment, language spoken, emotions of the speaker, age/gender of speaker etc. The two main steps in ASR are converting the audio file into features and classifying it appropriately. Basic unit of speech sound is phoneme and the list of such phoneme is language dependent. In Indian languages, basic unit of language is known as Akshara i.e the alphabet. It is known to be an alphasyllabary unit. In our work, we have analyzed the behavior of the acoustic features like, Mel frequency cepstral coefficients and linear predictive coding for various aksharas using techniques like, visualization, probability density function (pdf), Q–Q plot and F-ratio. The classifiers, support vector machine (SVM) and hidden Markov model (HMM) are used for classifying the recorded audio into corresponding aksharas. We have also compared the classification performance of HMM and SVM.},
author = {Hegde, Sarika and Achary, K. K. and Shetty, Surendra},
doi = {10.1007/s10772-014-9250-8},
issn = {15728110},
journal = {International Journal of Speech Technology},
keywords = {Alphasyllabary,Hidden Markov model,Kannada language,Linear predictive coding (LPC),Mel frequency cepstral coefficients (MFCC),Speech recognition,Statistical analysis,Support vector machine,Vector quantization (VQ)},
number = {1},
pages = {65--75},
publisher = {Kluwer Academic Publishers},
title = {{Statistical analysis of features and classification of alphasyllabary sounds in Kannada language}},
volume = {18},
year = {2014}
}
@inproceedings{Al-Thubaity2013,
abstract = {Feature selection is one of several factors affecting text classification systems. Feature selection aims to choose a representative subset of all features to reduce the complexity of classification problems. Usually a single method is used for feature selection. For English, several attempts were reported examining the combination of different feature selection methods. To the best of our knowledge no such attempts were reported for Arabic text classification. In this study, we examined the effect of combining five feature selection methods, namely CHI, IG, GSS, NGL and RS, on Arabic text classification accuracy. Two approaches of combination were used, intersection (AND) and union (OR). The NB classification algorithm was used to classify a Saudi Press Agency dataset which comprised 6,300 texts divided evenly into six classes. Three feature representation schemas were used, namely Boolean, TFiDF and LTC. The experiments show slight improvement in classification accuracy for combining two and three feature selection methods. No improvement on classification accuracy was seen when four or all five feature selection methods were combined. {\textcopyright} 2013 IEEE.},
author = {Al-Thubaity, Abdulmohsen and Abanumay, Norah and Al-Jerayyed, Sara and Alrukban, Aljoharah and Mannaa, Zarah},
booktitle = {SNPD 2013 - 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing},
doi = {10.1109/SNPD.2013.89},
isbn = {9780769550053},
keywords = {Arabic text classification,classification accuracy,classification algorithms,feature representation,feature selection},
pages = {211--216},
title = {{The effect of combining different feature selection methods on arabic text classification}},
year = {2013}
}
@article{Hall2009,
abstract = {Abstract More than twelve years have elapsed since the first public release of WEKA . In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread ... $\backslash$n},
author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
doi = {10.1145/1656274.1656278},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
month = {nov},
number = {1},
pages = {10},
publisher = {Association for Computing Machinery (ACM)},
title = {{The WEKA data mining software}},
volume = {11},
year = {2009}
}
@misc{Asuncion2007,
abstract = {The UCI Machine Learning Repository is a collection of databases, domain theories, and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms. The archive was created as an ftp archive in 1987 by David Aha and fellow graduate students at UC Irvine. Since that time, it has been widely used by students, educators, and researchers all over the world as a primary source of machine learning data sets. As an indication of the impact of the archive, it has been cited over 1000 times, making it one of the top 100 most cited "papers" in all of computer science. The current version of the web site was designed in 2007 by Arthur Asuncion and David Newman, and this project is in collaboration with Rexa.info at the University of Massachusetts Amherst. Funding support from the National Science Foundation is gratefully acknowledged.},
author = {Asuncion, A and Newman, D J},
booktitle = {University of California Irvine School of Information},
keywords = {data sets,machine learning,repositories},
title = {{UCI Machine Learning Repository: Data Sets}},
url = {https://archive.ics.uci.edu/ml/datasets.php http://www.ics.uci.edu/{~}mlearn/MLRepository.html{\%}5Cnhttp://archive.ics.uci.edu/ml/datasets.html},
urldate = {2019-12-08},
volume = {2008},
year = {2007}
}
