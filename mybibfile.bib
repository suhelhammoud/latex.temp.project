
@article{Kamalov2017,
abstract = {One of the major aspects of any classification process is selecting the rel-evant set of features to be used in a classification algorithm. This initial step in data analysis is called the feature selection process. Disposing of the irrelevant features from the dataset will reduce the complexity of the classification task and will increase the robustness of the decision rules when applied on the test set. This paper proposes a new filtering method that combines and normalizes the scores of three major feature selection methods: information gain, chi-squared statistic and inter-correlation. Our method utilizes the strengths of each of the aforementioned methods to maximum advantage while avoiding their drawbacks—especially the disparity of the results pro-duced by these methods. Our filtering method stabilizes each variable score and gives it the true rank among the input data's available variables. Hence it maximizes the stability in the variables' scores without losing the overall accuracy of the predictive model. A number of experiments on different datasets from various domains have shown that features chosen by the proposed method are highly predictive when com-pared with features selected by other existing filtering methods. The evaluation of the filtering phase was conducted via thorough experimentations using a number of predictive classification algorithms in addition to statistical analysis of the filtering methods' scores.},
author = {Kamalov, Firuz and Thabtah, Fadi},
doi = {10.1007/s40745-017-0116-1},
issn = {2198-5804},
journal = {Annals of Data Science},
mendeley-groups = {ins},
month = {dec},
number = {4},
pages = {483--502},
publisher = {Springer Science and Business Media LLC},
title = {{A Feature Selection Method Based on Ranked Vector Scores of Features for Classification}},
volume = {4},
year = {2017}
}
@article{Kamalov2017a,
abstract = {One of the major aspects of any classification process is selecting the rel-evant set of features to be used in a classification algorithm. This initial step in data analysis is called the feature selection process. Disposing of the irrelevant features from the dataset will reduce the complexity of the classification task and will increase the robustness of the decision rules when applied on the test set. This paper proposes a new filtering method that combines and normalizes the scores of three major feature selection methods: information gain, chi-squared statistic and inter-correlation. Our method utilizes the strengths of each of the aforementioned methods to maximum advantage while avoiding their drawbacks—especially the disparity of the results pro-duced by these methods. Our filtering method stabilizes each variable score and gives it the true rank among the input data's available variables. Hence it maximizes the stability in the variables' scores without losing the overall accuracy of the predictive model. A number of experiments on different datasets from various domains have shown that features chosen by the proposed method are highly predictive when com-pared with features selected by other existing filtering methods. The evaluation of the filtering phase was conducted via thorough experimentations using a number of predictive classification algorithms in addition to statistical analysis of the filtering methods' scores.},
author = {Kamalov, Firuz and Thabtah, Fadi},
doi = {10.1007/s40745-017-0116-1},
issn = {2198-5804},
journal = {Annals of Data Science},
mendeley-groups = {ins},
month = {dec},
number = {4},
pages = {483--502},
publisher = {Springer Science and Business Media LLC},
title = {{A Feature Selection Method Based on Ranked Vector Scores of Features for Classification}},
volume = {4},
year = {2017}
}
@article{McCluskey2014,
author = {McCluskey, Lee and Thabtah, Fadi and Mohammad, Rami M.},
doi = {10.1049/iet-ifs.2013.0202},
issn = {1751-8709},
journal = {IET Information Security},
mendeley-groups = {ins},
month = {may},
number = {3},
pages = {153--160},
title = {{Intelligent rule-based phishing websites classification}},
url = {https://digital-library.theiet.org/content/journals/10.1049/iet-ifs.2013.0202},
volume = {8},
year = {2014}
}
@article{Bunker2019,
author = {Bunker, Rory P. and Thabtah, Fadi},
doi = {10.1016/j.aci.2017.09.005},
file = {::},
issn = {22108327},
journal = {Applied Computing and Informatics},
mendeley-groups = {ins},
month = {jan},
number = {1},
pages = {27--33},
title = {{A machine learning framework for sport result prediction}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2210832717301485},
volume = {15},
year = {2019}
}
@article{Chandrashekar2014,
abstract = {Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Chandrashekar, Girish and Sahin, Ferat},
doi = {10.1016/j.compeleceng.2013.11.024},
issn = {00457906},
journal = {Computers and Electrical Engineering},
mendeley-groups = {ins},
month = {jan},
number = {1},
pages = {16--28},
title = {{A survey on feature selection methods}},
volume = {40},
year = {2014}
}
@inproceedings{Abdelhamid2017,
abstract = {In the last decade, numerous fake websites have been developed on the World Wide Web to mimic trusted websites, with the aim of stealing financial assets from users and organizations. This form of online attack is called phishing, and it has cost the online community and the various stakeholders hundreds of million Dollars. Therefore, effective counter measures that can accurately detect phishing are needed. Machine learning (ML) is a popular tool for data analysis and recently has shown promising results in combating phishing when contrasted with classic anti-phishing approaches, including awareness workshops, visualization and legal solutions. This article investigates ML techniques applicability to detect phishing attacks and describes their pros and cons. In particular, different types of ML techniques have been investigated to reveal the suitable options that can serve as anti-phishing tools. More importantly, we experimentally compare large numbers of ML techniques on real phishing datasets and with respect to different metrics. The purpose of the comparison is to reveal the advantages and disadvantages of ML predictive models and to show their actual performance when it comes to phishing attacks. The experimental results show that Covering approach models are more appropriate as anti-phishing solutions, especially for novice users, because of their simple yet effective knowledge bases in addition to their good phishing detection rate.},
author = {Abdelhamid, Neda and Thabtah, Fadi and Abdel-Jaber, Hussein},
booktitle = {2017 IEEE International Conference on Intelligence and Security Informatics: Security and Big Data, ISI 2017},
doi = {10.1109/ISI.2017.8004877},
isbn = {9781509067275},
keywords = {Computer Security,Machine Learning,Phishing Detection,Web Threat},
mendeley-groups = {ins},
month = {aug},
pages = {72--77},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Phishing detection: A recent intelligent machine learning comparison based on models content and features}},
year = {2017}
}
@article{Hall1999,
abstract = {A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational ...},
author = {Hall, Mark A.},
file = {::},
mendeley-groups = {ins},
number = {April},
title = {{Correlation-based Feature Selection for Machine Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.4643},
year = {1999}
}

